import sys
import os
import numpy as np
from tensorflow.keras.preprocessing.image import load_img
from tensorflow.keras import optimizers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dropout, Flatten, Dense, Activation, Input
from tensorflow.keras.layers import  Conv2D, MaxPooling2D
from tensorflow.keras import backend as K
from tensorflow.keras.models import Sequential, Model
import cv2
from matplotlib import pyplot as plt
%matplotlib inline
%config InlineBackend.figure_format='retina'

K.clear_session()

!git clone https://github.com/juanarango220485/FSS-CNN.git

import glob, os
import numpy as np
from tensorflow.keras.preprocessing.image import load_img
from scipy.ndimage import sobel

data = '/content/FSS-CNN/dataset'  # folder where the speckles are
os.chdir(data)
files = []  # "files" is a list (of strings) that will contain each of the NAMES of the images.

for file in glob.glob("*.tiff"):  # We read all .tiff files
    files.append(file)

num_datos = 601
dim = 126  # image dimension (length = width)
X = np.zeros(shape=[num_datos, dim, dim, 3], dtype=np.uint16)  # Empty numpy array to store speckle images
y = np.zeros(shape=[num_datos])  # Empty numpy array to store the corresponding temperature labels

# The entire dataset is stored in the 3 channels of the variable 'X' and in 'y' the corresponding labels.
for i in range(num_datos):
    ini = files[i].find('mm_') + 3  # We look for the temperature value incorporated in the nomenclature of each file
    fin = files[i].find('Â°C')
    y[i] = float(files[i][ini:fin])  # And we extract the temperature value from the string

    # Load the image in grayscale
    img = np.asarray(load_img(files[i], color_mode='grayscale', target_size=(dim, dim)), dtype=np.uint16)

    # Compute Sobel gradients
    sobel_x = sobel(img, axis=0)  # Gradient in the X direction
    sobel_y = sobel(img, axis=1)  # Gradient in the Y direction
    sobel_magnitude = np.hypot(sobel_x, sobel_y)  # Gradient magnitude

    # Store the original image and its Sobel gradients in the X array
    X[i, :, :, 0] = img  # Original image in the first channel
    X[i, :, :, 1] = sobel_x  # Sobel X gradient in the second channel
    X[i, :, :, 2] = sobel_magnitude  # Sobel magnitude in the third channel

# Now X contains the original image, its X gradient, and gradient magnitude, and y contains the labels.



# The dataset is divided into training (80%) and test (20%).
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)

import tensorflow as tf
from tensorflow.keras import layers

# Create patches
class Patches(layers.Layer):
    def __init__(self, patch_size):
        super(Patches, self).__init__()
        self.patch_size = patch_size

    def call(self, images):
        batch_size = tf.shape(images)[0]
        patches = tf.image.extract_patches(
            images=images,
            sizes=[1, self.patch_size, self.patch_size, 1],
            strides=[1, self.patch_size, self.patch_size, 1],
            rates=[1, 1, 1, 1],
            padding='VALID')
        patch_dims = patches.shape[-1]
        patches = tf.reshape(patches, [batch_size, -1, patch_dims])
        return patches

# Patch encoder
class PatchEncoder(layers.Layer):
    def __init__(self, num_patches, embedding_dim):
        super(PatchEncoder, self).__init__()
        self.num_patches = num_patches
        self.projection = layers.Dense(embedding_dim)
        self.position_embedding = layers.Embedding(
            input_dim=num_patches, output_dim=embedding_dim)

    def call(self, patches):
        positions = tf.range(start=0, limit=self.num_patches, delta=1)
        encoded = self.projection(patches) + self.position_embedding(positions)
        return encoded

# Transformer block
def transformer_block(inputs, embedding_dim, num_heads, mlp_dim, dropout_rate):
    x = layers.LayerNormalization(epsilon=1e-6)(inputs)
    attention_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)(x, x)
    x = layers.Add()([attention_output, inputs])
    x = layers.LayerNormalization(epsilon=1e-6)(x)
    mlp_output = layers.Dense(mlp_dim, activation=tf.nn.gelu)(x)
    mlp_output = layers.Dense(embedding_dim)(mlp_output)
    return layers.Add()([mlp_output, x])

# Prion memory mechanism
class PrionMemoryMechanism(layers.Layer):
    def __init__(self, embedding_dim):
        super(PrionMemoryMechanism, self).__init__()
        self.gate = layers.Dense(embedding_dim, activation="sigmoid")
        self.embedding_dim = embedding_dim
        self.memory_state = None

    def build(self, input_shape):
        self.memory_state = self.add_weight(
            shape=(input_shape[1], input_shape[2]),
            initializer="zeros",
            trainable=False,
            name="memory_state"
        )

    def call(self, inputs):
        gate_value = self.gate(inputs)
        updated_memory = gate_value * self.memory_state + (1 - gate_value) * inputs
        updated_memory_state = tf.reduce_mean(updated_memory, axis=0)
        self.memory_state.assign(updated_memory_state)
        return tf.broadcast_to(self.memory_state, tf.shape(inputs))

# Selective transformer block dropout
class SelectiveBlockDropout(layers.Layer):
    def __init__(self, dropout_rate):
        super(SelectiveBlockDropout, self).__init__()
        self.dropout_rate = dropout_rate

    def call(self, transformer_blocks, training=False):
        if training:
            block_mask = tf.random.uniform(shape=(len(transformer_blocks),), minval=0, maxval=1)
            block_mask = tf.cast(block_mask > self.dropout_rate, tf.float32)
        else:
            block_mask = tf.ones((len(transformer_blocks),), dtype=tf.float32)

        outputs = []
        for i, block in enumerate(transformer_blocks):
            output = block * block_mask[i]
            outputs.append(output)
        return tf.reduce_mean(tf.stack(outputs, axis=0), axis=0)

# ViT model with prion memory and selective block dropout
def create_vit_model(image_size, patch_size, num_patches, embedding_dim, num_heads,
                     mlp_dim, num_transformer_blocks, dropout_rate, block_dropout_rate):
    inputs = layers.Input(shape=(image_size, image_size, 3))

    patches = Patches(patch_size)(inputs)
    encoded_patches = PatchEncoder(num_patches, embedding_dim)(patches)

    prion_memory = PrionMemoryMechanism(embedding_dim)
    transformer_blocks = []
    x = encoded_patches
    for _ in range(num_transformer_blocks):
        x = transformer_block(x, embedding_dim, num_heads, mlp_dim, dropout_rate)
        transformer_blocks.append(x)
        x = prion_memory(x)

    selective_dropout = SelectiveBlockDropout(block_dropout_rate)
    final_representation = selective_dropout(transformer_blocks, training=True)

    x = layers.LayerNormalization(epsilon=1e-6)(final_representation)
    h = w = int(num_patches ** 0.5)
    x = layers.Reshape((h, w, embedding_dim))(x)

    # CNN head for regression task
    x = layers.Conv2D(filters=128, kernel_size=3, padding='same', activation='relu')(x)
    x = layers.MaxPooling2D(pool_size=2)(x)
    x = layers.Conv2D(filters=256, kernel_size=3, padding='same', activation='relu')(x)
    x = layers.GlobalAveragePooling2D()(x)
    output = layers.Dense(1, activation='linear')(x)  # Single output for regression (temperature)

    model = tf.keras.Model(inputs=inputs, outputs=output)
    return model

# Parameters
image_size = 126  # FSS image size (126x126)
patch_size = 14   # Adjust patch size to fit the image dimensions
num_patches = (image_size // patch_size) ** 2  # 8x8 patches
embedding_dim = 128
num_heads = 4
mlp_dim = 256
num_transformer_blocks = 8
dropout_rate = 0.1
block_dropout_rate = 0.1

# Create the model
vit_prion_model = create_vit_model(image_size, patch_size, num_patches, embedding_dim,
                                   num_heads, mlp_dim, num_transformer_blocks,
                                   dropout_rate, block_dropout_rate)

# Compile with Mean Squared Error for regression
vit_prion_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0008),
                        loss='mean_squared_error',
                        metrics=['mae'])

# Train the model with FSS dataset
vit_prion_model.fit(X_train, y_train, batch_size=32, epochs=100, validation_data=(X_test, y_test))
